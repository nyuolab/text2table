from datetime import date
from text2table.metrics.exact_match import get_wrong_char
import evaluate

@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)
class MainMetric(datasets.Metric):
    """TODO: Short description of my metric."""

    def _info(self):
        # TODO: Specifies the datasets.MetricInfo object
        return datasets.MetricInfo(
            # This is the description that will appear on the metrics page.
            description=_DESCRIPTION,
            citation=_CITATION,
            inputs_description=_KWARGS_DESCRIPTION,
            # This defines the format of each prediction and reference
            
            # Note: we have to define the features as simply 'predictions' and 'reference' instead of our actuall dataset features (column names) which will lead to a ValueError. (Since huggingface will assume that the column name are the actual columns being passed into ref/pred, which isn't what we're doing here)
            
            features=datasets.Features({
                'predictions': datasets.Value('string'),
                'references': datasets.Value('string'),
            }),
            # Homepage of the metric for documentation
            homepage="http://metric.homepage",
            # Additional links to the codebase or references
            codebase_urls=["http://github.com/path/to/codebase/of/new_metric"],
            reference_urls=["http://path.to.reference.url/new_metric"]
        )


    def matching_pair(result,cel_pred,cel_ref,category_token):
        # evluation within cell: single or mul values with '<CEL>'
        # check categories
        if category_token in ['<DOB>']: #time, single
            # add diff days
            result[category_token]+=evaluate.load('date_metric.py').compute(predictions=cel_pred,references=cel_ref)
        elif category_token in ['<GENDER>','<HOSPITAL_EXPIRE_FLAG>']: # binary (singlelabel.py), single
            # directly concatenate
            result[category_token]['pred']+=cel_pred
            result[category_token]['ref']+=cel_ref 
        elif category_token in ['<DIAG_ICD9>','<PROC_ICD9>','<CPT_CD>']: # hierarchy, multi
            # --unsure: matching
            if category_token=='<DIAG_ICD9>':
                # --- Gavin's icd9
        elif category_token in ['<PRESCRIPTION>','<DRG_CODE>']: # char exact match, multi
            pred_assigned,ref_assigned=evaluate.load('exact_match.py').compute(mode=mode,cel_pred=cel_pred,cel_ref=cel_ref)
            assert(len(pred_assigned)==len(ref_assigned))
            result[category_token]['pred']+=pred_assigned
            result[category_token]['ref']+=ref_assigned
        elif category_token in ['LAB_MEASUREMENT']:
            # --unsure: problem
            # parse
            for ele_pred,ele_ref in zip(cel_pred,cel_ref):
                part_ref=cel_ref.split(" ")
                part_pred=cel_pred.split(" ")
                # id: exact match
                error_id=get_wrong_char(part_pred[0],part_ref[0])
                # value: int
                error_val=abs(part_pred[1]-part_ref[1])
                # unit: exact match
                error_unit=get_wrong_char(part_pred[2],part_ref[2])
                
                if len(part_ref)==4: # also has flag
                    # flag: 
                    if part_pred[2]==part_ref[2]:
                        error_flag==0
                    else:
                        error_flag==1
                
                # weight and sum:
                if len(part_ref)==4:
                    error_sum=np.sum([error_id,error_val,error_unit,error_flag])/4
                elif len(part_ref)==3:
                    error_sum=np.sum([error_id,error_val,error_unit])/3

        # append result
        result[category_token]+=error
        return result


    def _compute(self, predictions, references,inputs): #predictions, references both in a batch
        """Returns the scores"""
        # #log config:
        # os.makedirs('eval_logs',exist_ok=True)
        # date=datetime.datetime.now()
        # n=date.strftime("eval_logs/%m_%d_%H:%M:%S_eval.log")
        
        #initiate dictionary
        result={}
        
        
        for i in [f'<{x}>' for x in ['DIAG_ICD9', 'PROC_ICD9', 'PRESCRIPTION', 'DRG_CODE','LAB_MEASUREMENT','HOSPITAL_EXPIRE_FLAG', 'GENDER', 'CPT_CD','DOB']]:
            result[i]=0
            # --unsure: need further revision depending on what's the metric generated by each metric file
        # initiate ways to record errors
        result['label_mismatch']=0
        
        #iterate thru rows/inputs
        for row_pred,row_ref,row_input in zip(predictions, references,inputs):
            # check single vs multi label
            if ' <CEL> ' in row_ref: # multi label
                if ' <CEL> ' not in row_pred: # error: prediction only has 1 label
                    print('error: supposed to be multi label, but prediction only generates single label.')
                     result['label_mismatch']+=1
                    continue 
                cel_pred=row_pred[i].split(' <CEL> ')
                cel_ref=row_ref[i].split(' <CEL> ')
            else: # single label
                cel_pred=[row_pred[i]]
                cel_ref=[row_ref[i]]
            
            #call cel_match helper function
            sub_result=cel_match(mode=mode,cel_pred=cel_pred,cel_ref=cel_ref)

            # --unsure: append subresult to result
            for i in sub_result:
                result[i]+=1
        
        # check categories for final metric calculation
        if category_token in ['<DOB>']: #time
            # average diff days
            result[category_token]+=evaluate.load('date_metric.py').compute(predictions=cel_pred,references=cel_ref)
        elif category_token in ['<GENDER>','<HOSPITAL_EXPIRE_FLAG>']: # binary (singlelabel.py)
            result[category_token]['pred']+=cel_pred
            result[category_token]['pred']+=cel_ref 
        elif category_token in ['<DIAG_ICD9>','<PROC_ICD9>','<CPT_CD>']: # hierarchy
            error=evaluate.load('multilabel.py').compute(predictions=cel_pred,references=cel_ref)
        elif category_token in ['<PRESCRIPTION>','<DRG_CODE>']: # multi-char exact match (col_wise_metric_script.py)
        error=evaluate.load('singlelabel.py').compute(predictions=cel_pred,references=cel_ref)


        # --unsure: any further flatening of dictionary
        final=result

        return final

