tokenizer: 
  # the directory where the pretokenized data are stored
  ptk_dir_train: '../data/pretokenized/train/'
  ptk_dir_val: '../data/pretokenized/val/'
  max_input_length: 8192
  max_output_length: 512
  max_header_length: 512
  batch_size: 32
  # Change num_cpu for different envirnoments
  num_cpu: 32
model:
  num_beams: 2
  max_length: 512
  min_length: 0
  length_penalty: 1.0
  early_stopping: false
trainer:
  use_decoder_header: false
  gradient_checkpointing: true
  output_dir: '../../models/'
  predict_with_generate: true
  evaluation_strategy: 'steps'
  # Change training/evaluation batch size based on needs
  #--changed
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  fp16: true
  logging_steps: 10
  #--changed
  eval_steps: 1000
  #eval_steps: 1
  #--changed
  #max_steps: 2
  save_steps: 1000
  save_total_limit: 2
  # --changed
  gradient_accumulation_steps: 2
  # Change name for different runs
  run_name: "run"
  group: "mvp_f1_final_3"
