tokenizer: 
  # the directory where the pretokenized data are stored
  ptk_dir_train: '../data/pretokenized/train/'
  ptk_dir_val: '../data/pretokenized/val/'
  max_input_length: 8192
  max_output_length: 512
  max_header_length: 512
  batch_size: 32
  # Change num_cpu for different envirnoments
  num_cpu: 40
model:
  num_beams: 2
  max_length: 512
  min_length: 0
  length_penalty: 1.0
  early_stopping: false
trainer:
  use_decoder_header: true
  gradient_checkpointing: true
  output_dir: '../../models/'
  predict_with_generate: true
  evaluation_strategy: 'steps'
  # Change training/evaluation batch size based on needs
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  fp16: true
  logging_steps: 10
  eval_steps: 2000
  save_steps: 2000
  save_total_limit: 2
  gradient_accumulation_steps: 2
  wandb_mode: "disabled"
  # Change name for different runs
  run_name: "text2table-trail-#"
  group: "experiment-#"
  debug: false
dataset:
  version: 'full'