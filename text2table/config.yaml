tokenizer: 
  # the directory where the pretokenized data are stored
  ptk_dir_train: '../data/pretokenized/train/'
  ptk_dir_val: '../data/pretokenized/val/'
  max_input_length: 8192
  max_output_length: 512
  max_header_length: 512
  batch_size: 32
  # Change num_cpu for different envirnoments
  num_cpu: 40
model:
  num_beams: 2
  max_length: 512
  min_length: 0
  length_penalty: 1.0
  early_stopping: false
trainer:
  use_decoder_header: true
  gradient_checkpointing: true
  output_dir: '../../models/'
  predict_with_generate: false
  prediction_loss_only: true
  evaluation_strategy: 'steps'
  # Change training/evaluation batch size based on needs
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 1
  fp16: true
  logging_steps: 20
  eval_steps: 3000
  eval_accumulation_steps: 1
  save_steps: 3000
  save_total_limit: 10
  gradient_accumulation_steps: 2
  wandb_mode: "online"
  # Change name for different runs
  run_name: "v4_thread_"
  group: "multinode_led_base_v4"
  debug: false
  deepspeed: "../ds_config.json"
dataset:
  version: 'full'